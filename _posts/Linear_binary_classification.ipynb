{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "409f263f-6e2b-45e6-9ced-5342d75ee1f0",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Supervised learning is a branch of machine learning in which an algorithm is trained on labelled inputâ€“output pairs. The goal is to learn a mapping from inputs to outputs that generalises well to unseen data. When the outputs belong to a small, finite set of categories, the task is called **classification**. When the outputs come from an ordered or continuous set, the task is known as **regression**.\n",
    "\n",
    "This note focuses on the **perceptron algorithm**, a foundational method for solving linear binary classification problems.\n",
    "\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## Linear Binary Classification\n",
    "Consider a training set \n",
    "\\begin{equation}\n",
    "\\left\\{ \\left({\\mathbf{x}}_i, y_i\\right) \\right\\}^N_{i=1},\n",
    "\\end{equation} \n",
    "where each ${\\mathbf{x}}_i$ is a feature vector and each label $y_i$ takes values in $\\left\\{-1, +1\\right\\}$. The goal is to find a linear function that maps feature vectors to labels. We consider classifiers of the form\n",
    "\\begin{equation}\n",
    "f\\left({\\mathbf{x}}\\right) = {\\mathbf{w}}^{\\top}{{\\mathbf{x}}}+ w_0=\\hat{\\mathbf{w}}^{\\top}\\hat{\\mathbf{x}},\n",
    "\\tag{1}\\end{equation}\n",
    "where $\\hat{\\mathbf{x}}=\\left[{\\mathbf{x}}, 1\\right]^{\\top}$ and $\\hat{\\mathbf{w}}=\\left[{\\mathbf{w}}, w_0\\right]^{\\top}$ is a vector of real-valued parameters. Different choices of $\\hat{\\mathbf{w}}$ produce different linear functions, each corresponding to a hyperplane that attempts to separate the two classes in the training data.\n",
    "\n",
    "To evaluate a particular classifier, we measure its **training error**:\n",
    "\\begin{equation}\n",
    "\\frac{1}{N}\\sum_{i=1}^N \\mathbf{1}\\!\\left[f(\\mathbf{x}_i) y_i \\le 0 \\right],\n",
    "\\tag{2}\\end{equation}\n",
    "where the indicator function $\\mathbf{1}[\\cdot]$ equals $1$ if the condition is true and $0$ otherwise. This quantity counts the proportion of misclassified training examples.\n",
    "\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## Perceptron Algorithm\n",
    "The perceptron algorithm seeks a parameter vector $\\hat{\\mathbf{w}}$ that minimises the number of misclassifications. A training example $\\left({\\mathbf{x}}_i, y_i\\right)$ is considered **misclassified** when\n",
    "\\begin{equation}\n",
    "y_i\\hat{\\mathbf{w}}^\\top\\hat{\\mathbf{x}}_i\\leq 0.\n",
    "\\tag{3}\\end{equation}\n",
    "\n",
    "The algorithm iterates through the training set and updates the parameters only when a misclassification occurs. The update rule is\n",
    "\\begin{equation}\n",
    "{\\hat{\\mathbf{w}}}\\gets \\hat{\\mathbf{w}}+ y_i\\hat{\\mathbf{x}}_i. \n",
    "\\tag{4}\\end{equation}\n",
    "To see why this corrects the mistake, consider evaluating the classifier on the same input after an update:\n",
    "\\begin{equation}\n",
    "y_i\\hat{\\mathbf{w}}^\\top\\hat{\\mathbf{x}}_i\\quad \\text{increases by}\\quad \\lVert{\\hat{\\mathbf{x}}_i\\rVert}^2.\n",
    "\\tag{5}\\end{equation}\n",
    "Thus, repeated updates push the classifier toward correctly classifying ${\\mathbf{x}}_i$. Misclassification of other examples may push $\\hat{\\mathbf{w}}$ in different directions, but the algorithm stops once all examples are correctly classified.\n",
    "\n",
    "If the data are linearly separable, the perceptron algorithm converges after a finite number of updates. For proof of convergence, see, e.g., Jaakkola (2006).\n",
    "\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## Learning Algorithm: Perceptron\n",
    "**Initialize**:\n",
    "\\begin{equation}\n",
    "\\hat{\\mathbf{w}}=\\mathbf{0}\n",
    "\\end{equation}\n",
    "\n",
    "**Repeat until all examples are correctly classified**:\n",
    "1. For each training example $\\left({\\mathbf{x}}_i, y_i\\right)$:\n",
    "   - if $y_i\\hat{\\mathbf{w}}^\\top\\hat{\\mathbf{x}}_i\\leq 0$, update\n",
    "     \\begin{equation}{\\hat{\\mathbf{w}}}\\gets \\hat{\\mathbf{w}}+ y_i\\hat{\\mathbf{x}}_i\\end{equation}\n",
    "     \n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## Implementation with NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da796a-f88d-4c18-a5d5-c71bb485a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def perceptron_train(X, y, max_epochs=1000):\n",
    "    \"\"\"\n",
    "    Train a perceptron classifier.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        Training feature vectors.\n",
    "    y : ndarray of shape (n_samples,)\n",
    "        Class labels in {-1, +1}.\n",
    "    max_epochs : int, optional\n",
    "        Maximum number of passes over the training set.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    w : ndarray of shape (n_features,)\n",
    "        Learned weight vector.\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    # Initialize weights to zero\n",
    "    w = np.zeros(n_features, dtype=float)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        errors = 0\n",
    "\n",
    "        for i in range(n_samples):\n",
    "            # Check if the current example is misclassified\n",
    "            if y[i] * np.dot(w, X[i]) <= 0:\n",
    "                # Update rule: w <- w + y_i * x_i\n",
    "                w += y[i] * X[i]\n",
    "                errors += 1\n",
    "\n",
    "        # Stop if there are no misclassifications in this epoch\n",
    "        if errors == 0:\n",
    "            break\n",
    "\n",
    "    return w\n",
    "\n",
    "\n",
    "def perceptron_predict(X, w):\n",
    "    \"\"\"\n",
    "    Predict labels for input data using a trained perceptron.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : ndarray of shape (n_samples, n_features)\n",
    "        Input feature vectors.\n",
    "    w : ndarray of shape (n_features,)\n",
    "        Weight vector learned by perceptron_train.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    y_pred : ndarray of shape (n_samples,)\n",
    "        Predicted labels in {-1, +1}.\n",
    "    \"\"\"\n",
    "    scores = X @ w\n",
    "    # Map scores to {-1, +1}\n",
    "    return np.where(scores >= 0, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b897189e-d75a-40f9-ba48-4d0695f7c9f0",
   "metadata": {},
   "source": [
    "Example application need to come here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cee3aa0-f08e-4777-83f7-38b4139f5837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: (n_samples, n_features), y: labels in {-1, +1}\n",
    "w = perceptron_train(X, y)\n",
    "y_pred = perceptron_predict(X, w)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.22"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
